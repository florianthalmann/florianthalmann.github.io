<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-04-24T01:32:16+01:00</updated><id>http://localhost:4000/</id><title type="html">Florian Thalmann</title><subtitle>A site about my research, music, and software.</subtitle><entry><title type="html">Semantic Machine</title><link href="http://localhost:4000/2019/03/30/machine.html" rel="alternate" type="text/html" title="Semantic Machine" /><published>2019-03-30T14:23:58+00:00</published><updated>2019-03-30T14:23:58+00:00</updated><id>http://localhost:4000/2019/03/30/machine</id><content type="html" xml:base="http://localhost:4000/2019/03/30/machine.html">&lt;p&gt;The Semantic Machine is an contextual composition created in collaboration with
Tracy Redhead. Via mobile sensors and data taken from public APIs, the work adjusts to the listener’s context and varies its musical characteristics depending on location, time of the day and weather. We used traditional music production tools as well as the Semantic Player, a dynamic music web framework which allows data manipulation techniques familiar from computer music and the video game industry.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/projects/semantic-machine.png&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">The Semantic Machine is an contextual composition created in collaboration with Tracy Redhead. Via mobile sensors and data taken from public APIs, the work adjusts to the listener’s context and varies its musical characteristics depending on location, time of the day and weather. We used traditional music production tools as well as the Semantic Player, a dynamic music web framework which allows data manipulation techniques familiar from computer music and the video game industry.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/projects/semantic-machine.png" /></entry><entry><title type="html">Plunderphonics</title><link href="http://localhost:4000/2019/03/30/plunderphonics.html" rel="alternate" type="text/html" title="Plunderphonics" /><published>2019-03-30T14:23:58+00:00</published><updated>2019-03-30T14:23:58+00:00</updated><id>http://localhost:4000/2019/03/30/plunderphonics</id><content type="html" xml:base="http://localhost:4000/2019/03/30/plunderphonics.html">&lt;p&gt;Plunderphonics is a web application that automatically creates intelligent
collages from music available in the Live Music Archive of the Internet Archive.
For example, it arranges all the different versions of a particular song from
chronologically and generates a diachronic transsection which is smoothly mixed
together based on automatic music analysis while displaying
artefacts from the corresponding live concerts gathered from various web platforms
and linked together using Semantic Web technologies.
&lt;a href=&quot;https://grateful-dead-live.github.io/plunderphonics&quot; target=&quot;blank&quot;&gt;Live&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/projects/plunderphonics.png&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">Plunderphonics is a web application that automatically creates intelligent collages from music available in the Live Music Archive of the Internet Archive. For example, it arranges all the different versions of a particular song from chronologically and generates a diachronic transsection which is smoothly mixed together based on automatic music analysis while displaying artefacts from the corresponding live concerts gathered from various web platforms and linked together using Semantic Web technologies. Live</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/projects/plunderphonics.png" /></entry><entry><title type="html">Play it Again! Use it Together</title><link href="http://localhost:4000/2018/09/14/piauit.html" rel="alternate" type="text/html" title="Play it Again! Use it Together" /><published>2018-09-14T15:23:58+01:00</published><updated>2018-09-14T15:23:58+01:00</updated><id>http://localhost:4000/2018/09/14/piauit</id><content type="html" xml:base="http://localhost:4000/2018/09/14/piauit.html">&lt;p&gt;Web application for the exhibition Play it Again! Use it Together with the Open Music Archive at Victoria Gallery in Liverpool, where Shellac records are being digitised, analyzed automatically, and fed into an archiving system based on Semantic Web technologies. A Node server generates an infinite growing composition created from recently digitized material and sent to all clients, one of which also plays in the exhibition.
&lt;a href=&quot;https://www.playitagainuseittogether.com&quot; target=&quot;blank&quot;&gt;Live&lt;/a&gt;
/
&lt;a href=&quot;https://github.com/open-music-archive&quot; target=&quot;blank&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/projects/piauit.png&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">Web application for the exhibition Play it Again! Use it Together with the Open Music Archive at Victoria Gallery in Liverpool, where Shellac records are being digitised, analyzed automatically, and fed into an archiving system based on Semantic Web technologies. A Node server generates an infinite growing composition created from recently digitized material and sent to all clients, one of which also plays in the exhibition. Live / GitHub</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/projects/piauit.png" /></entry><entry><title type="html">Audio Dream</title><link href="http://localhost:4000/2017/09/09/audiodream.html" rel="alternate" type="text/html" title="Audio Dream" /><published>2017-09-09T23:23:58+01:00</published><updated>2017-09-09T23:23:58+01:00</updated><id>http://localhost:4000/2017/09/09/audiodream</id><content type="html" xml:base="http://localhost:4000/2017/09/09/audiodream.html">&lt;p&gt;Audio Dream is a web-based system based on deep neural networks that listens to 
a number of improvising live musicians, learns from their playing, and plays
along with them using reorganized fragments of the decomposed recorded material.&lt;/p&gt;

&lt;p&gt;(Recordings available soon)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/projects/audio-dream.jpg&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/projects/audio-dream2.jpg&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">Audio Dream is a web-based system based on deep neural networks that listens to a number of improvising live musicians, learns from their playing, and plays along with them using reorganized fragments of the decomposed recorded material.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/projects/audio-dream.jpg" /></entry><entry><title type="html">Owleapyo</title><link href="http://localhost:4000/2017/09/06/owleapyo.html" rel="alternate" type="text/html" title="Owleapyo" /><published>2017-09-06T15:23:58+01:00</published><updated>2017-09-06T15:23:58+01:00</updated><id>http://localhost:4000/2017/09/06/owleapyo</id><content type="html" xml:base="http://localhost:4000/2017/09/06/owleapyo.html">&lt;p&gt;Owleapyo is a Python live performance system based on semantic technologies
and deep learning which listens to and interacts with a live performer using
a number of controllers, including a modified version of the Ableton Push
and the Leap Motion. It is based on a multidimensional model of musical
expression capable of capturing and sending MIDI and sensor data.&lt;/p&gt;

&lt;p&gt;(Recordings available soon)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/projects/owleapyo.png&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">Owleapyo is a Python live performance system based on semantic technologies and deep learning which listens to and interacts with a live performer using a number of controllers, including a modified version of the Ableton Push and the Leap Motion. It is based on a multidimensional model of musical expression capable of capturing and sending MIDI and sensor data.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/projects/owleapyo.png" /></entry><entry><title type="html">Fast DJ</title><link href="http://localhost:4000/2016/09/06/fastdj.html" rel="alternate" type="text/html" title="Fast DJ" /><published>2016-09-06T15:23:58+01:00</published><updated>2016-09-06T15:23:58+01:00</updated><id>http://localhost:4000/2016/09/06/fastdj</id><content type="html" xml:base="http://localhost:4000/2016/09/06/fastdj.html">&lt;p&gt;An automatic web-based minimal-UI automated DJing application that adapts to the user’s preference via simple interactive decisions and feedback on taste. Starting from a preset or custom-defined decision tree modeled on common DJ practice, the system can gradually learn a more customized and user-specific tree. At the core of the system are structural representations of the musical content based on semantic audio technologies and inferred from features extracted from the audio directly in the browser.
&lt;a href=&quot;https://dynamic-music.github.io/fast-dj&quot; target=&quot;blank&quot;&gt;Live&lt;/a&gt;
/
&lt;a href=&quot;https://github.com/dynamic-music/fast-dj&quot; target=&quot;blank&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/projects/fast-dj.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/projects/dj.jpg&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">An automatic web-based minimal-UI automated DJing application that adapts to the user’s preference via simple interactive decisions and feedback on taste. Starting from a preset or custom-defined decision tree modeled on common DJ practice, the system can gradually learn a more customized and user-specific tree. At the core of the system are structural representations of the musical content based on semantic audio technologies and inferred from features extracted from the audio directly in the browser. Live / GitHub</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/projects/dj.jpg" /></entry><entry><title type="html">Grateful Live</title><link href="http://localhost:4000/2016/09/06/gratefullive.html" rel="alternate" type="text/html" title="Grateful Live" /><published>2016-09-06T15:23:58+01:00</published><updated>2016-09-06T15:23:58+01:00</updated><id>http://localhost:4000/2016/09/06/gratefullive</id><content type="html" xml:base="http://localhost:4000/2016/09/06/gratefullive.html">&lt;p&gt;A platform for the representation and discovery of live music recordings and associated artefacts of the band The Grateful Dead. It links material from the Grateful Dead collection of the Internet Archive with data aggregated from several additional Web resources discussing and describing these events. These data include descriptions and images of physical artefacts such as tickets, posters and fan photos, as well as other information, e.g. about location and weather. The system uses signal processing techniques for the analysis and alignment of the digital recordings. During the discovery, users can juxtapose and compare different recordings of a given concert, or different performances of a given song by interactively blending between them.
&lt;a href=&quot;https://grateful-dead-live.github.io&quot; target=&quot;blank&quot;&gt;Live&lt;/a&gt;
/
&lt;a href=&quot;https://github.com/grateful-dead-live&quot; target=&quot;blank&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/projects/grateful-live.png&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">A platform for the representation and discovery of live music recordings and associated artefacts of the band The Grateful Dead. It links material from the Grateful Dead collection of the Internet Archive with data aggregated from several additional Web resources discussing and describing these events. These data include descriptions and images of physical artefacts such as tickets, posters and fan photos, as well as other information, e.g. about location and weather. The system uses signal processing techniques for the analysis and alignment of the digital recordings. During the discovery, users can juxtapose and compare different recordings of a given concert, or different performances of a given song by interactively blending between them. Live / GitHub</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/projects/grateful-live.png" /></entry><entry><title type="html">Performance Deformations</title><link href="http://localhost:4000/2016/08/10/perfdef.html" rel="alternate" type="text/html" title="Performance Deformations" /><published>2016-08-10T23:23:58+01:00</published><updated>2016-08-10T23:23:58+01:00</updated><id>http://localhost:4000/2016/08/10/perfdef</id><content type="html" xml:base="http://localhost:4000/2016/08/10/perfdef.html">&lt;p&gt;An educational web application for comparative performance analysis based on source separation and object-based audio techniques. The underlying system decomposes recordings of classical music performances into note events using score-informed source separation and represents the decomposed material using semantic web technologies. In a visual and interactive way, users can explore individual performances by highlighting specific musical aspects directly within the audio and by altering the temporal characteristics to obtain versions in which the micro-timing is exaggerated or suppressed. Multiple performances of the same work can be compared by juxtaposing and blending between the corresponding recordings. Finally, by adjusting the timing of events, users can generate intermediates of multiple performances to investigate their commonalities and differences.
&lt;a href=&quot;https://github.com/florianthalmann/performance-playground&quot; target=&quot;blank&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/projects/performance-playground.png&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">An educational web application for comparative performance analysis based on source separation and object-based audio techniques. The underlying system decomposes recordings of classical music performances into note events using score-informed source separation and represents the decomposed material using semantic web technologies. In a visual and interactive way, users can explore individual performances by highlighting specific musical aspects directly within the audio and by altering the temporal characteristics to obtain versions in which the micro-timing is exaggerated or suppressed. Multiple performances of the same work can be compared by juxtaposing and blending between the corresponding recordings. Finally, by adjusting the timing of events, users can generate intermediates of multiple performances to investigate their commonalities and differences. GitHub</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/projects/performance-playground.png" /></entry><entry><title type="html">Dymo Designer</title><link href="http://localhost:4000/2015/04/06/dymodesigner.html" rel="alternate" type="text/html" title="Dymo Designer" /><published>2015-04-06T15:23:58+01:00</published><updated>2015-04-06T15:23:58+01:00</updated><id>http://localhost:4000/2015/04/06/dymodesigner</id><content type="html" xml:base="http://localhost:4000/2015/04/06/dymodesigner.html">&lt;p&gt;A prototypical Web app for creating and analyzing Dynamic Music Objects in a visual, interactive, and computer-assisted way. Implemented using Angular, D3, and the Web Audio API.
&lt;a href=&quot;https://github.com/florianthalmann/dymo-designer&quot; target=&quot;blank&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/projects/dymo-designer.png&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">A prototypical Web app for creating and analyzing Dynamic Music Objects in a visual, interactive, and computer-assisted way. Implemented using Angular, D3, and the Web Audio API. GitHub</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/projects/dymo-designer.png" /></entry><entry><title type="html">Harmony of the Spheres</title><link href="http://localhost:4000/2015/03/10/spheres.html" rel="alternate" type="text/html" title="Harmony of the Spheres" /><published>2015-03-10T22:23:58+00:00</published><updated>2015-03-10T22:23:58+00:00</updated><id>http://localhost:4000/2015/03/10/spheres</id><content type="html" xml:base="http://localhost:4000/2015/03/10/spheres.html">&lt;p&gt;An Android app that investigates the sonic potential of objects in n-dimensional physical spaces with transforming properties. Using gestural multi-touch and accelerometer control, users can create musical objects in these spaces and interact with them, while they move and react to the physical conditions of the spaces. The properties of these objects can be arbitrarily mapped to sound parameters, either of an internal synthesizer or external systems, and they can be visualized in flexible ways. On a larger scale, users can make soundscapes by defining sequences of physical space conditions, each of which has an effect on the motion and properties of the physical objects.
&lt;a href=&quot;https://play.google.com/store/apps/details?id=org.flobot.hots&quot; target=&quot;blank&quot;&gt;Google Play&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/projects/hots.png&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">An Android app that investigates the sonic potential of objects in n-dimensional physical spaces with transforming properties. Using gestural multi-touch and accelerometer control, users can create musical objects in these spaces and interact with them, while they move and react to the physical conditions of the spaces. The properties of these objects can be arbitrarily mapped to sound parameters, either of an internal synthesizer or external systems, and they can be visualized in flexible ways. On a larger scale, users can make soundscapes by defining sequences of physical space conditions, each of which has an effect on the motion and properties of the physical objects. Google Play</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/projects/hots.png" /></entry></feed>